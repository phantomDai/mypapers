\documentclass[10pt,journal,compsoc]{IEEEtran}
% \UseRawInputEncoding
\usepackage{enumerate}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
%\interdisplaylinepenalty=2500
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{ulem}
\usepackage{booktabs}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Dynamic Random Testing of Web Services: \\ A Methodology and Evaluation}


\author{Chang-ai~Sun,~\IEEEmembership{Senior Member,~IEEE,}
        Hepeng~Dai,
        Guan~Wang,
        Dave~Towey,~\IEEEmembership{Member,~IEEE,}
        Tsong Yueh~Chen,~\IEEEmembership{Member,~IEEE,}
        and~Kai-Yuan~Cai
\thanks{A preliminary version of this paper was presented at the 36th Annual IEEE Computer Software and Applications Conference (COMPSAC 2012)~\cite{sun2012towards}.}
\thanks{C.-A. Sun, H. Dai, and G. Wang are with the School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China. E-mail: casun@ustb.edu.cn.}% <-this % stops a space
\thanks{D. Towey is with the School of Computer Science, University of Nottingham Ningbo China, Ningbo 315100, China. E-mail: dave.towey@nottingham.edu.cn.}
\thanks{T.Y. Chen is with the Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn VIC 3122, Australia. Email: tychen@swin.edu.au.}
\thanks{K.-Y. Cai is with the School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China. E-mail: kycai@buaa.edu.cn.}% <-this % stops a
}


\markboth{IEEE TRANSACTIONS ON SERVICES COMPUTING,~submitted}%
{Sun \MakeLowercase{\textit{Sun et al.}}: An Empirical Study on Dynamic Random Testing of Web Services: A Methodology and Evaluation}


\IEEEtitleabstractindextext{
\begin{abstract}
  In recent years, Service Oriented Architecture (SOA) has been increasingly adopted to develop distributed applications in the context of the Internet. To develop reliable SOA-based applications, an important issue is how to ensure the quality of web services. In this paper, we propose a dynamic random testing (DRT) technique for web services, which is an improvement over the widely-practiced random testing (RT)~and partition testing (PT) approaches. We examine key issues when adapting DRT to the context of SOA, including a framework, guidelines for parameter settings, and a prototype for such an adaptation. Empirical studies are reported where DRT is used to test three real-life web services, and mutation analysis is employed to measure the effectiveness. Our experimental results show that, compared with the three baseline techniques, RT, Adaptive Testing (AT) and Random Partition Testing (RPT), DRT demonstrates higher fault-detection effectiveness with a lower test case selection overhead. Furthermore, the theoretical guidelines of parameter setting for DRT are confirmed to be effective. The proposed DRT and the prototype provide an effective and efficient approach for testing web services.
\end{abstract}

\begin{IEEEkeywords}
Software Testing, Random Testing, Dynamic Random Testing, Web Service, Service Oriented Architecture.
\end{IEEEkeywords}}

\maketitle

\IEEEdisplaynontitleabstractindextext


\IEEEpeerreviewmaketitle


\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{S}{ervice} oriented architecture (SOA)~\cite{papazoglou2008service} defines a loosely coupled, standards-based, service-oriented application development paradigm in the context of the Internet.
Within SOA, three key roles are defined:
service providers (who  develop and own services);
service requestors (who consume or invoke services); and
a service registry (that registers services from providers and returns services to requestors).
Applications are built upon services that present functionalities through publishing their interfaces in appropriate repositories, abstracting away from the underlying implementation.
Published interfaces may be searched by other services or users, and then invoked.
Web services are the realization of SOA based on open standards and infrastructures~\cite{sun2011transaction}.
Ensuring the reliability of SOA-based applications can become critical when such applications implement important business processes.

Software testing is a practical method for ensuring the quality and reliability of software.
However, some SOA features can pose challenges for the testing of web services~\cite{bartolini2009whitening, canfora}.
For instance, service requestors often do not have access to the source code of web services which are published and owned by another organization, and, consequently, it is not possible to use white-box testing techniques.
Testers may, therefore, naturally turn to black-box testing techniques.

Random Testing (RT)~\cite{Hamlet02} is one of the most widely-practiced black-box testing techniques.
Because test cases in RT are randomly selected from the input domain (which refers to the set of all possible inputs of the software under test), it can be easy to implement.
Nevertheless, because RT does not make use of any information about the software under test (SUT), or the test history, it may be inefficient in some situations.
In recent years, many efforts have been made to improve RT in different ways~\cite{cai2009random, chen2010adaptive, Cai07}.
Adaptive random testing (ART)
\textcolor{blue}{[8], [10]--[14]}
\cite{chen2010adaptive, 8846002},
 for example, has been proposed to improve RT by attempting to have a more diverse distribution of test cases in the input domain.

In contrast to RT, partition testing (PT) attempts to generate test cases in a more ``systematic'' way, aiming, to use fewer test cases to reveal more faults.
When conducting PT, the input domain of the SUT is divided into disjoint partitions, with test cases then selected from each and every one.
Each partition is expected to have a certain degree of homogeneity---test cases in the same partition should have similar software execution behavior.
Ideally, a partition should also be homogeneous in fault detection:
If one input can reveal a fault, then all other inputs in the same partition should also be able to reveal a fault.

RT and PT are based on different intuitions, and each have their own advantages and disadvantages.
Because it is likely that they can be complementary to each other, detecting different faults, it is intuitively appealing to investigate their integration in random partition testing (RPT).

In traditional RPT \cite{cai2009random}, the partitions and corresponding test profiles remain constant throughout testing, which may not be the best strategy.
Independent researchers have observed that fault-revealing inputs tend to cluster into ``continuous regions''~\cite{Ammann88, Finelli91}---there is similarity in the execution behavior of neighboring software inputs.
Based on software cybernetics, Cai et al. proposed adaptive testing (AT) to control the testing process \cite{cai2004optimal}, however, AT's decision-making incurs an extra computational overhead. To alleviate this, dynamic random testing (DRT) \cite{cai2009random} was proposed by Cai et al., aiming to improve on both RT and RPT.

In practice, web services have usually been tested by the service providers, and simple or easy-to-test faults have been removed, meaning that the remaining faults are normally hard to detect.
For ensuring a higher reliability of the web services, a simple RT strategy may not be appropriate \cite{li2014two}, especially when the scale is large, or there are some stubborn faults.

In this paper, we present a DRT approach for web services, as an enhanced version of RT adapted to the context of SOA.
We examine key issues of such an adaptation, and, accordingly, propose a framework for testing web services that combines the principles of DRT \cite{cai2009random} and the features of web services. To validate the fault detection effectiveness and efficiency of the proposed DRT method in the context of SOA, we conduct a comprehensive empirical study. We also explore the impact factors of the proposed DRT, and provide guidelines for setting DRT parameters based on a theoretical analysis. Finally, we compare the performance of the proposed DRT with other baseline techniques.

This paper extends our previous work \cite{sun2012towards} in the following aspects. Firstly, this paper extensively examines the challenges and practical situations related to testing web services (Section \ref{sec:webservices}). It also extensively discusses the limitations of RT, PT, RPT, and AT, when they are used for testing web services (Section \ref{sec:introduction}). Secondly, although previous work \cite{sun2012towards} provided a coarse-grained framework for DRT of web services, PT was not studied. In contrast, this paper provides a comprehensive solution based on partitioning (Section 4.4.1). Thirdly, based on a theoretical analysis (Section 3.2), this paper provides guidelines for setting DRT parameters. Such guidelines are crucial to enhance the practical application of DRT, which was not covered in previous work \cite{sun2012towards}. Fourthly, previous work \cite{sun2012towards} only evaluated the fault detection effectiveness and efficiency of the proposed approach (DRT) in terms of the F-measure and T-measure, and only two small web services (ATM Service and Warehouse Service) were used in the evaluation of its performance. This paper, in contrast, provides a comprehensive evaluation that not only evaluates the fault detection effectiveness of the proposed approach in terms of the F-measure, F2-measure, and T-measure (Section \ref{sec:RQ1}), but also evaluates its efficiency in terms of F-time, F2-time, and T-time (Section \ref{sec:RQ3}). Furthermore, we also examine three real-life web services, comparing the fault-detection effectiveness and efficiency of the proposed approach with those of RT, RPT, and AT. Statistical analysis is used to validate the significance of the empirical evaluations and comparisons (Sections \ref{sec:RQ1} and \ref{sec:RQ3}), which was not covered in previous work \cite{sun2012towards}. Extending again the previous work \cite{sun2012towards}, we also examine the relationship between the number of partitions and the optimal control parameter settings for DRT, evaluating the usefulness of guidelines provided by the theoretical analysis (Section \ref{sec:RQ2}). The contributions of this work, combined with previous work \cite{sun2012towards}, include:

\begin{itemize}
  \item
  We develop an effective and efficient testing method for web services.
  This includes a DRT framework that addresses key issues for testing web services, and a prototype that partly automates the framework.
  \item
  We evaluate the performance of DRT through a series of empirical studies on three real web services.
  These studies show that DRT has significantly higher fault-detection efficiency than RT and RPT. That is, to detect a given number of faults, DRT uses less time and fewer test cases than RT and RPT.
  \item
  We provide guidelines for the DRT parameter settings, supported by theoretical analysis, and validated by the empirical studies.
\end{itemize}

The rest of this paper is organized as follows.
Section~\ref{sec:background} introduces the underlying concepts for DRT, and web services.
Section~\ref{sec:DRTforwebserivce} presents the DRT framework for web services, guidelines for its parameter settings, and a prototype that partially automates DRT.
Section~\ref{sec:empiricalstudy} describes an empirical study where the proposed DRT is used to test three real-life web services, the results of which are summarized in Section~\ref{sec:results}.
Section~\ref{sec:relatedwork} discusses related work and Section~\ref{sec:conclusion} concludes the paper.

\section{Background}
\label{sec:background}

In this section, we present some of the underlying concepts for DRT, and web services.

\subsection{Dynamic Random Testing (DRT)}
\label{sec:DRTStrategy}

DRT combines RT and PT, with the goal of benefitting from the advantages of both.
Given a test suite \emph{TS} classified into $m$ partitions (denoted $s_1, s_2, \ldots, s_m$),  suppose that a test case from $s_i$ ($i = 1, 2, \ldots, m$) is selected and executed.
If this test case reveals a fault, $\forall j = 1, 2, \ldots, m$ and $j \neq i$, we then set

\begin{equation}
\label{eq:DRThitJ}
p'_j =
\begin{cases}
p_j - \displaystyle\frac{\varepsilon}{m-1} & \text{if } p_j \geq \displaystyle\frac{\varepsilon}{m-1} \\
0 & \text{if } p_j < \displaystyle\frac{\varepsilon}{m-1}
\end{cases},
\end{equation}
where $\varepsilon$ is a probability adjusting factor, and then

\begin{equation}
\label{eq:DRThitI}
  p'_i = 1 - \sum_{\substack{j = 1 \\ j \neq i}}^m p'_j.
\end{equation}
Alternatively, if the test case does not reveal a fault, we set

\begin{equation}
\label{eq:DRTmissI}
p'_i =
\begin{cases}
p_i - \varepsilon & \text{if } p_i \geq \varepsilon \\
0 & \text{if } p_i < \varepsilon
\end{cases},
\end{equation}
and then for $\forall j = 1, 2, \ldots, m$ and $j \neq i$, we set

\begin{equation}
\label{eq:DRTmissJ}
p'_j =
\begin{cases}
p_j + \displaystyle\frac{\varepsilon}{m-1} & \text{if } p_i \geq \varepsilon \\
p_j + \displaystyle\frac{p'_i}{m-1} & \text{if } p_i < \varepsilon
\end{cases}.
\end{equation}

Algorithm \ref{alg:DRT} describes DRT.
In DRT, the first test case is taken from a partition that has been randomly selected according to the initial probability profile $\{p_1, p_2, \ldots, p_m\}$ (Lines 2 and 3 in Algorithm~\ref{alg:DRT}).
After each test case execution, the test profile $\{ \left \langle s_1,p_1 \right \rangle, \left \langle s_2,p_2 \right \rangle, \ldots, \left \langle s_m,p_m \right \rangle\}$ is updated by changing the values of $p_i$:
If a fault is revealed, Formulas~\ref{eq:DRThitJ} and~\ref{eq:DRThitI} are used;
otherwise, Formulas~\ref{eq:DRTmissI} and~\ref{eq:DRTmissJ} are used.
The updated test profile is then used to guide the random selection of the next test case (Line 8).
This process is repeated until a termination condition is satisfied (Line 1).
Examples of possible termination conditions include:
 ``testing resources have been exhausted'';
 ``a certain number of test cases have been executed''; and
 ``a certain number of faults have been detected".

\begin{algorithm}
    \caption{DRT}
    \label{alg:DRT}
    \begin{algorithmic}[1]
        \renewcommand{\algorithmicrequire}{\textbf{Input:}} %Ê¹µÃÔ­À´Èí¼þ°üÖÐ¶¨ÒåµÄÃüÁî\REQUIRE ºÍ\ENSUREÏÔÊ¾ÎªInput:ºÍOutput:
	    \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \renewcommand{\algorithmicendwhile}{\algorithmicend\_\algorithmicwhile}
		\renewcommand{\algorithmicendfor}{\algorithmicend\_\algorithmicfor}
		\renewcommand{\algorithmicendif}{\algorithmicend\_\algorithmicif}
		\renewcommand{\algorithmicthen}{}
		\renewcommand{\algorithmicdo}{}
        \REQUIRE $\varepsilon, p_1, p_2, \ldots, p_m$
        \WHILE {termination condition is not satisfied}
        \STATE Select a partition $s_i$ according to the testing profile $\{ \left \langle s_1,p_1 \right \rangle, \left \langle s_2,p_2 \right \rangle, \ldots, \left \langle s_m,p_m \right \rangle\}$.
        \STATE Select a test case $t$ from $s_i$.
        \STATE Test the software using $t$.
        \IF {a fault is revealed by $t$}
        \STATE Update $p_j$ ($j = 1, 2, \ldots, m$ and $j \neq i$) and $p_i$ according to Formulas~\ref{eq:DRThitJ} and~\ref{eq:DRThitI}.
        \ELSE
        \STATE Update $p_j$ ($j = 1, 2, \ldots, m$ and $j \neq i$) and $p_i$ according to Formulas~\ref{eq:DRTmissI} and~\ref{eq:DRTmissJ}.
        \ENDIF
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

As can be seen from Formulas~\ref{eq:DRThitJ} to~\ref{eq:DRTmissJ},  updating the test profile involves $m$ simple calculations, thus requiring a constant time.
Furthermore, the selection of partition $s_i$, and subsequent selection and execution of the test case, all involve a constant time.
The execution time for one iteration of DRT is thus a constant, and therefore the overall time complexity for DRT to select $n$ test cases is $O(m \cdot n)$.

\subsection{Web Services}
\label{sec:webservices}

A web service is a platform-independent, loosely coupled, self-contained, programmable, web-enabled application that can be described, published, discovered, coordinated and configured using XML artifacts for the purpose of developing distributed interoperable applications~\cite{papazoglou2008service}.
A web service consists of a description (usually specified in WSDL) and implementation (written in any programming language).
Web services present their functionalities through published interfaces, and are usually deployed in a service container.
Invocation of a web service requires analysis of the input message in its WSDL, test data generation based on its input parameters, and wrapping of test data in a SOAP message.

A web service is a basic component of SOA software, and, accordingly, the reliability of such SOA software depends heavily on the quality of the component web services.
While testing is an obvious potential activity to help assuring the quality of web services, due to the unique features of SOA, web service testing can be more challenging than traditional software testing \cite{canfora}.
Some of these features include:
\begin{itemize}
  \item
  \emph{Lack of access to service implementation:}
  Normally, web service owners will not make the source code of their web services accessible.
  Typically, service users only have access to the service interface defined in a WSDL file, which means that white-box testing approaches are not possible.
  \item
  \emph{Incomplete documentation or specification:}
  A service provider may only offer an incomplete or inaccurate description of a service's functional and non-functional behavior.
  This makes it difficult to decide whether or not a test passes, especially when details about behavior or restrictions on implementations are missing \cite{sun2018constraint}.
  \item
  \emph{Lack of control:}
  Unlike traditional software testing where testers can control the execution of the software under test, there is usually no opportunity to intervene in the execution of the web service under test, which is often deployed in a remote service container.

  \item
  \emph{Side effects caused by testing:}
  A large number of tests may introduce an additional communication load, and hence impact on the performance of the web service under test.
  This suggests that the number of tests should be kept as low as possible \cite{sunreview}.
\end{itemize}

\section{DRT for Web Services}
\label{sec:DRTforwebserivce}

In this section, we describe a framework for applying DRT to web services, discuss guidelines for setting DRT's parameters, and present a prototype that partially automates  DRT for web services.

\subsection{Framework}
\label{sec:framework}

Considering the principles of DRT and the features of web services, we propose a DRT for web services framework, as illustrated in Figure~\ref{fig:frame}.
In the figure, the DRT components are inside the box, and the web services under test and testers are located outside.
Interactions between DRT components, the web services, and testers are depicted in the framework.
We next discuss the individual framework components.

\begin{figure}[]
  \centering
  \includegraphics[width = 0.5\textwidth]{fig//framework.pdf}
  \caption{DRT for web services framework}
  \label{fig:frame}
\end{figure}

\begin{enumerate}[1)]
  \item
  \emph{WSDL Parsing}.
  Web services are composed of services and the relevant WSDL documents.
  By parsing the WSDL document, we can get the input information for each operation in the services.
  This includes the number of parameters, their names and types, and any additional requirements that they may have.

  \item
  \emph{Partition Construction}.
  Partition testing (PT) refers to a class of testing techniques that classify the input domain into a number of partitions~\cite{weyuker1991analyzing}.
  Because DRT is a black-box testing technique, combining RT and PT, the PT approaches used are at the specification level.
  Various approaches and principles for achieving convenient and effective partitions have been discussed in the literature
   \textcolor{blue}{[21]--[24]}
   \cite{weyuker1991analyzing, chen1994relationship}.
  The input domain of the web service under test (WSUT) can be partitioned based on the WSUT specifications and the parsed parameters.
  Once partitioned, testers can assign probability distributions to the partitions as an initial testing profile.
  This initial testing profile can be assigned in different ways, including using a uniform probability distribution, or one that sets probabilities according to the importance of the partition:
  For example, a partition within which faults were previously detected should be given higher priority.

  \item
  \emph{Test Profile and DRT Parameter Initialization}.
  Testers need to initialize the test profile, a simple way of doing which would be the use of a uniform probability distribution ($p_1 = p_2 = \ldots = p_k = 1 / k$, where $k$ denotes the number of partitions, and $p_i$ $(i = 1,2, \ldots, k)$ denotes the probability of selecting the $i^{th}$ partition). They also need to set the DRT parameters (guidelines for which are introduced in Section \ref{sec:relation}).

  \item
  \emph{Partition Selection}.
  DRT randomly selects a partition $s_i$ according to the test profile.

  \item
  \emph{Test Case Selection}.
  DRT selects a test case from the selected partition $s_i$ according to a uniform distribution.

  \item
  \emph{Test Case Execution}.
  The relevant DRT component receives the generated test case, converts it into an input message, invokes the web service(s) through the SOAP protocol, and intercepts the test results (from the output message).

  \item
  \emph{Test Profile Adjustment}.
  Upon completion of each test, its pass or fail status is determined by comparing the actual and expected results (a pass status if both are the same).
  The pass or fail status is then used to adjust the (partition) probability distribution accordingly.
  Situations where determination of the test outcome status is not possible (i.e. in the presence of the oracle problem
  \textcolor{blue}{[25]--[28]}
  \cite{barr2015oracle, segura2018metamorphic})
   may potentially be addressed using metamorphic testing~\cite{sun2011}.
\end{enumerate}

Generally speaking, DRT test case generation is influenced by both the probability distribution (for selection of the relevant partition), and the principles of RT
---
combining the effectiveness of PT with the ease of RT.
Because our technique is based on PT, it is necessary that the partition details be provided (by the tester), which can easily be done through analysis of the input parameters and their constraints, as described in the specification of the web service under test. Once the partition details are available, then it is not difficult to set an initial test profile. The tester can, for example, simply use a uniform probability distribution ($p_1 = p_2 = \ldots = p_m = 1 / m$, where $m$ denotes the number of partitions, and $p_i $ $(i = 1,2,\ldots,m)$ denotes the probability of selecting the $i^{th}$ partition). In Section \ref{sec:relation}, we provide some guidelines for how to set the DRT parameters.
Furthermore, many of the components in the DRT for web services framework can be automated.
To make it easier for potential adopters of DRT for web services, we have also developed a prototype application (described in Section \ref{sec:prototype}).

\subsection{Guidelines for Parameter Setting}
\label{sec:relation}

Our previous work~\cite{sun2012towards} found that the DRT performance depends on the number of partitions and the parameter $\varepsilon$.
We next explore these impacts through a theoretical analysis, which, to be mathematically tractable, has the following assumptions:

\begin{enumerate}[1)]
  \item
  The failure rate $\theta_i$ of each partition $s_i$ ($i = 1, 2, \ldots, m$, and $m > 1$) is unknown, but can be estimated.

  \item
  Each failure rate~$\theta_i$ ($i = 1, 2, \ldots, m$, and $m > 1$) remains unchanged throughout the testing process (faults are not removed after their detection).

  \item
  Test cases are selected with replacement, which means that some test cases may be selected more than once.
\end{enumerate}

A principle of the DRT strategy is to increase the selection probabilities (by amount $\varepsilon$) of partitions with larger failure rates.
In addition to the impact of the parameter $\varepsilon$, the number of partitions also influences the speed of updating the test profile (Formulas \ref{eq:DRThitJ} to \ref{eq:DRTmissJ}).
Therefore, for a given number of partitions, we are interested in investigating what values of $\varepsilon$ yield the best DRT performance.

Letting $\theta_M$ denote the maximum failure rate, and $s_M$ denote partitions with that failure rate, then $p_i^n$ denotes the probability of executing the $n^{th}$ test case from partition $s_i$.
As testing proceeds, the probability $p_M$ of partition $s_M$ being selected is expected to increase:
\begin{equation}
    \label{eq:exception}
    p_M^{n + 1} > p_M^{n}
\end{equation}

In order to achieve the best performance, the probability of selecting the partition $s_M$ (which has the maximum failure rate) is expected to increase. To achieve this, the increase in probability of $s_M$ being selected for the next round should be larger than that for other partitions. We further analyze sufficient conditions for this goal, and can accordingly derive an interval for $\varepsilon$.

Initially, the test profile is
$\{ \left \langle s_1,p_1^0 \right \rangle, \left \langle s_2,p_2^0 \right \rangle, \ldots, \left \langle s_m,p_m^0 \right \rangle\}$,
which, after $n$ test cases have been executed,  is then updated to
$\{ \left \langle s_1,p_1^n \right \rangle, \left \langle s_2,p_2^n \right \rangle, \ldots, \left \langle s_m,p_m^n \right \rangle\}$.
During the testing process, $p_i^n$ is increased or decreased by the value $\varepsilon$, which is relatively small (set to $0.05$ in previous studies \cite{Lv2011,li2015}).
Because the initial $p_i^0$ is larger than $\varepsilon$, and the adjustment of $p_i$ is relatively small (Formulas~\ref{eq:DRThitJ} to~\ref{eq:DRTmissJ}), the following two situations are rare, and thus not considered here:
$p_i < \varepsilon / (m - 1)$ or
$p_i < \varepsilon$ ($i = 1, 2, \ldots, m$).

To explore the relationship between $p_i^{n + 1}$ and $p_i^{n}$, we calculate the conditional probability, $p(i|\delta)$, of the following four situations (denoted as $\delta_1, \delta_2, \delta_3$, and $\delta_4$):

\begin{enumerate}[ {Situation} 1 ($\delta_1$):]
  \item
  \textbf{If $t_{n} \notin s_i$ and a fault is detected by $t_n$}, then $p(i|\delta_1)$ is calculated according to Formula \ref{eq:DRThitJ}:
    $$p(i|\delta_1) = \sum_{i \neq j}\theta_j(p_i^n - \displaystyle\frac{\varepsilon}{m - 1}).$$

  \item
  \textbf{If $t_{n} \in s_i$ and a fault is detected by $t_n$}, then $p(i|\delta_2)$ is calculated according to Formula \ref{eq:DRThitI}:
    $$p(i|\delta_2) = \theta_i(p_i^n + \varepsilon).$$

  \item
  \textbf{If $t_{n} \in s_i$ and no fault is detected by $t_n$}, then $p(i|\delta_3)$ is calculated according to Formula \ref{eq:DRTmissI}:
    $$p(i|\delta_3) = (1 - \theta_i)(p_i^n - \varepsilon).$$

  \item
  \textbf{If $t_{n} \notin s_i$ and no fault is detected by $t_n$}, then $p(i|\delta_4)$ is calculated according to Formula \ref{eq:DRTmissJ}:
    $$p(i|\delta_4) = \sum_{i \neq j}(1 - \theta_j)(p_i^n + \displaystyle\frac{\varepsilon}{m - 1}).$$
\end{enumerate}

Therefore, $p_i^{n + 1}$ for all cases together is:

\begin{equation}
    \label{eq:7}
    \begin{split}
    p_i^{n + 1}
    =&p_i^n + Y_i^n,
    \end{split}
\end{equation}
where
\begin{equation}
    \begin{split}
    \label{eq:yi}
    Y_i^n = &\displaystyle\frac{\varepsilon}{m - 1}(2p_i^n\theta_im - p_i^nm - 2p_i^n\theta_i + 1 )\\
             &- \displaystyle\frac{2\varepsilon}{m-1}\sum_{j \neq i}p_j^n\theta_j.
    \end{split}
 \end{equation}

From Formula~\ref{eq:yi}, we have:
 \begin{equation}
    \begin{split}
    \label{eq:26}
    Y_M^n - Y_i^n
    =&\displaystyle\frac{2\varepsilon}{m - 1}(m(p_M^n\theta_M - p_i^n\theta_i) - \displaystyle\frac{m(p_M^n - p_i^n)}{2}).
    \end{split}
\end{equation}

Before presenting the final guidelines, we need the following lemma.
\newtheorem{lem}{Lemma}
\label{Lemma}
\begin{lem}
  If $p_i^{n} - p_M^{n} > 2(p_i^{n}\theta_i - p_M^{n}\theta_M)$, then $p_M^{n + 1} > p_M^{n}$.
\end{lem}
\begin{IEEEproof}
See Appendix A.
\end{IEEEproof}

\vspace{1ex}
Accordingly, we can now present the following theorem that states a sufficient condition for achieving $p_M^{n + 1} > p_M^{n}$.
\newtheorem{theo}{Theorem}
\label{theorem}
\begin{theo}
  For failure rate $\theta_{min} = min\{\theta_1, \ldots, \theta_m\}$, $\theta_M > \theta_{min}$, if $0 < \theta_{min} < \frac{1}{2}$, the following condition is sufficient to guarantee that $p_M^{n + 1} > p_M^{n}$:
\begin{equation}
\label{equa:results}
  \displaystyle\frac{2m\theta_{min}^2}{1-2\theta_{min}} < \varepsilon < \displaystyle\frac{(m-1)m\theta_{min}}{2(m + 1)}.
\end{equation}
\end{theo}
\begin{IEEEproof}
See Appendix A.
\end{IEEEproof}

\vspace{1ex}
In summary, when $\frac{1}{2} < \theta_M < 1$, there is always an interval $\mathcal{I}$:
\begin{equation}
  \varepsilon \in (\displaystyle\frac{2m\theta_{min}^2}{1 - 2\theta_{min}}, \displaystyle\frac{(m - 1)m\theta_{min}}{2(m + 1)})
\end{equation}
where $\theta_{min} \le \theta_i, i \in \{1, 2, \ldots, m\}$, and $\theta_i \ne 0$, which can guarantee $p_M^{n+1} > p_M^n$.

\vspace{1ex}
From the proof in Appendix A, it is clear that the value of $\theta_M$ affects the upper bound ($\mathcal{I}_{upper}$) of $\mathcal{I}$.
When $\theta_{min} < \theta_M < \frac{1}{2}$, the value of $\mathcal{I}_{upper}$ should be close to the lower bound of $\mathcal{I}$.
In practice, we should set
\begin{equation}
\label{euqtion:approxValue}
  \varepsilon \approx \frac{2m\theta_{min}^2}{1-2\theta_{min}}.
\end{equation}

For a given partition scheme, with a total of $m$ partitions, identification of the partition with the minimum failure rate $(\theta_{min})$ first requires calculation of the failure rates of each partition, then identification of the minimum. Each partition's failure rate can be obtained in two ways:
\begin{enumerate}[1)]
  \item
  It can be calculated directly as $F / E$ ($F$ is the number of failures and $E$ is the number of executed tests), if the test history of the web service under test is available.

  \item
  It can be approximated by $1 / k_i$, where $k_i$ is the total number of test cases executed before revealing a fault.
\end{enumerate}

\subsection{Prototype}
\label{sec:prototype}

This section describes a tool that partially automates DRT for web services, called DRTester\footnote{The prototype tool, together with a number of accompanying resources, has been made available at: https://githup.com/phantomDai/DRTester.git}. DRTester supports the following tasks in testing web services: a) WSDL parsing; b) partition construction; c) setting DRT parameters (probability adjusting factor $\varepsilon$ and test profiles); and d) test case preparation and execution. The details of DRTester are as follows:

\begin{enumerate}[1)]
  \item
  \emph{Guidance}. This feature describes the steps the tester should follow when testing a web service.

  \item
  \emph{Configuration}. This feature, as shown in Figure 2, interacts with the testers to obtain and set the information related to testing the web service, including: the address of the web service under test; the DRT parameters and partition details; and the test case preparation. The detailed steps are as follows:

  \begin{itemize}
    \item
    \emph{Inputting and parsing the URL (Figure \ref{fig:configuration}.a)}: We integrate the WSDL parsing functionality provided by MT4WS \cite{sun2016mt4ws}. This enables all the (WSDL) parameters and their types to be automatically obtained.
    \item
    \emph{Parameter setting (Figure \ref{fig:configuration}.a)}: The tester is responsible for selecting which operations of the current web service under test are to be tested, and for partitioning each parameter into disjoint choices.
    \item
    \emph{Partition setting (Figure \ref{fig:configuration}.b)}: The tester is responsible for specifying the partitions by combining the choices associated with each parameter.
    \item
    \emph{Test case generation (Figure \ref{fig:configuration}.b)}: The tester is responsible for specifying the mode of test case generation (either randomly generating test cases based on the parameters, or uploading test cases generated using other techniques).
  \end{itemize}
  \item
  \emph{Execution}. This feature presents a summary of the testing results, including details of the test case execution (input, expected output, partition, and result (pass or fail)). For randomly generated tests, the tester has to check each individual result. Otherwise, when all tests have been completed, a report is generated in a downloadable file.
\end{enumerate}

\begin{figure}
  \centering
  \subfigure[WSDL Parsing and Parameters Setting]{\includegraphics[width=0.49\textwidth,height=8cm]{fig/parameter.pdf}}
  \subfigure[Partition and DRT Parameter Setting]{\includegraphics[width=0.49\textwidth,height=9cm]{fig/partition.pdf}}
  \caption{DRTester configuration snapshots}
  \label{fig:configuration}
\end{figure}

The back-end logic is composed of several Restful APIs and Java classes: The APIs are responsible for communicating HTTP messages to and from the front-end interface. The controller class is responsible for updating the test profile according to the test results, and for selecting test cases from the partitions. The selected test cases are wrapped in SOAP messages and sent to the web service under test through the proxy class, which also intercepts the test results.

\section{Empirical Study}
\label{sec:empiricalstudy}

We conducted a series of empirical studies to evaluate the performance of DRT.

\subsection{Research Questions}
\label{sec:questions}

In our experiments, we focused on addressing the following three research questions:

\begin{description}
  \item [RQ1] How effective is DRT at detecting web service faults?

  Fault-detection effectiveness is a key criterion for evaluating the performance of a testing technique.
  This study used three popular real-life web services as subject programs, and applied mutation analysis to evaluate the effectiveness.

  \item [RQ2] How do the number of partitions and the DRT parameter $\varepsilon$ impact on the failure detection effectiveness and efficiency of DRT?

  In our earlier work~\cite{sun2012towards}, we found that the DRT parameter $\varepsilon$ had a significant effect on DRT efficiency, and  that the optimal value of the parameter could be related to the number of partitions.
  The relationship between $\varepsilon$ and the number of partitions is examined through theoretical analysis, and verified through the empirical studies.

  \item [RQ3]

  Compared with the baseline techniques, how efficient is DRT at detecting web service faults in terms of time?

  Compared with RT and PT, DRT incorporates the selection of partitions and test cases within a partition. Compared with AT, which also introduces feedback and adaptive control principles to software testing, DRT has a simple but efficient control strategy. Thus, we are interested in comparing the fault detection efficiency of DRT, RT, PT, and AT in terms of their time costs.
  %To answer this, we compare the time cost of DRT with that of the baseline techniques.



\end{description}

\subsection{Subject Web Services}
\label{sec:subjects}

Although a number of web services are publicly available, for various reasons, their implementations are not. This renders them unsuitable for our experiments, which involve the creation of faulty mutants (requiring access to the implementations). We therefore selected three web services as the subject programs for our study, and implemented them ourselves, based on real-life specification\footnote{The implementations have been made available at: https://github.com/phantomDai/subjects4tsc.git}:
\texttt{Aviation Consignment Management Service (ACMS)};
\texttt{China Unicom billing service (CUBS)}; and
\texttt{Parking billing service (PBS)}. We used the tool MuJava \cite{ma2005mujava} to conduct mutation analysis
\textcolor{blue}{[34]--[36]}
\cite{demillo1978hints, sun2017path},
 generating a total of 1563 mutants. Each mutant was created by applying a syntactic change (using one of all applicable mutation operators provided by MuJava) to the original program. Equivalent mutants, and those that were too easily detected
(requiring less than 20 randomly generated test cases), were removed.
To ensure the statistical reliability, we obtained 50 different test suites using different random seeds, then tested all mutants with all test suites, calculating the average number of test cases needed to kill (detect) a mutant. Table~\ref{tab:objects} summarizes the basic information of the used web services and their mutants.
A detailed description of each web service is given in the following.

% table 1
\begin{table}[h]
\caption{Subject Web Services}
\label{tab:objects}
\centering
\begin{tabular}{lll} \toprule
  Web service                            &LOC                            &Number of mutants    \\ \midrule
  ACMS                                   &116                            &3                                    \\
  CUBS                                   &131                            &11                                  \\
  PBS                                    &129                            &4                                   \\ \bottomrule
\end{tabular}
\end{table}

% table 2
\begin{table*}
  \caption{\texttt{ACMS} Baggage Allowance and Pricing Rules}
  \label{tab:aviation}
  \centering
  \begin{tabular}{lllllll} \toprule
  \multirow{2}{*}{}     &\multicolumn{3}{l}{Domestic flights} &\multicolumn{3}{l}{International flights} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}
                       &First class  &Business class   &Economy class   &First class  &Business class   &Economy class \\ \midrule
  Carry~on (kg)   &5       &5        &5        &7        &7        &7  \\
  Free checked-in (kg)   &40      &30       &20       &40       &30       &20/30 \\
  Additional baggage pricing~(kg)   &\multicolumn{6}{c}{$weight \times price \times 1.5\%$}     \\ \bottomrule
  \end{tabular}
\end{table*}

\subsubsection{Aviation Consignment Management Service (ACMS)}
\label{sec:acms}

\texttt{ACMS} helps airline companies check the allowance (weight) of free baggage, and the cost of additional baggage.
Based on the destination, flights are categorised as either domestic or international.
For international flights, the baggage allowance is greater if the passenger is a student (30kg), otherwise it is 20kg.
Each aircraft offers three cabin classes from which to choose (economy, business, and first), with passengers in different classes having different allowances.
The detailed price rules are summarized in Table~\ref{tab:aviation}, where $price$ means economy class fare and $weight$ is the weight that exceeds the weight of the free carry.

\subsubsection{China Unicom Billing Service (CUBS)}
\label{sec:cubs}

\texttt{CUBS} provides an interface through which customers can know how much they need to pay according to cell-phone plans, calls, and data usage.
The details of several cell-phone plans are summarized in Tables~\ref{table:chinaC},~\ref{table:chinaB}, and~\ref{table:chinaA}.

% table 3
\begin{table}[h]
  \caption{Plan A}
  \label{table:chinaC}
  \centering
  \setlength{\tabcolsep}{1mm}{
  \begin{tabular}{lllll} \toprule
  \multicolumn{2}{l}{\multirow{2}{*}{Plan details}}  &\multicolumn{3}{l}{Month charge~(CNY)} \\ \cmidrule(lr){3-5}
  \multicolumn{2}{l}{}                                  &$op_A^1$  &$op_A^2$  &$op_A^3$ \\ \midrule
   \multirow{3}{*}{Basic} &Free calls~(min)  &260  &380  &550 \\
                                             &Free data~(MB)    &40         &60                 &80 \\
                                             &Free incoming calls  &\multicolumn{3}{l}{Domestic~(including video calls)} \\ \midrule
   \multirow{3}{*}{Extra} &Incoming calls~(CNY/min)             &0.25       &0.20               &0.15 \\
                                             &Data~(CNY/KB)     &3E-4       &3E-4               &3E-4 \\
                                             &Video calls~(CNY/min) &0.60   &0.60               &0.60 \\ \bottomrule
  \end{tabular}}
\end{table}

% table 4
\begin{table}[hbtp]
  \caption{Plan B}
  \label{table:chinaB}
  \centering
\setlength{\tabcolsep}{1mm}{
  \begin{tabular}{llllllll} \toprule
    \multicolumn{2}{l}{\multirow{2}{*}{Plan details}}   &\multicolumn{6}{l}{Month charge~(CNY)} \\ \cmidrule(lr){3-8}
    \multicolumn{2}{l}{}       &\!$op_B^1$\!  &\!$op_B^2$\!  &\!$op_B^3$\! &\!$op_B^4$\! &\!$op_B^5$\!  \!&$op_B^6$\!   \\ \midrule
    \multirow{3}{*}{Basic}                    &Free calls~(min)           &120   &200  &450  &680  &920  &1180 \\
                                              &Free data~(MB)             &40    &60   &80   &100  &120  &150 \\
                                              &Free incoming calls  &\multicolumn{6}{c}{Domestic~(including video calls)} \\ \midrule
    \multirow{3}{*}{Extra}                    &Incoming calls~(CNY/min)   &0.25  &0.20 &0.15 &0.15 &0.15 &0.15 \\
                                              &Data~(CNY/KB)              &3E-4  &3E-4 &3E-4 &3E-4 &3E-4 &3E-4 \\
                                              &Video calls~(CNY/min)      &0.60  &0.60 &0.60 &0.60 &0.60 &0.60\\ \bottomrule
  \end{tabular}}
\end{table}


\subsubsection{Parking Billing Service (PBS)}
\label{sec:pbs}

Consider a parking billing service that accepts the parking details for a vehicle, including the vehicle type, day of the week, discount coupon, and hours of parking.
This service  rounds up the parking duration to the next full hour, and then calculates the parking fee according to the hourly rates in Table~\ref{tab:hourlyRate}.
If a discount voucher is presented, a 50\% discount off the parking fee is applied.

To facilitate better parking management, at the time of parking, customers may provide an estimation of parking duration, in terms of three different time ranges ($(0.0,2.0]$, $(2.0,4.0]$, and $(4.0,24.0]$).
If the estimation and actual parked hours fall into the same time range, then the customer will receive a 40\% discount;
but if they are different ranges, then a 20\% markup is applied.
A customer may choose to either use a discount coupon, or provide an estimation of parking duration, but may not do both.
No vehicles are allowed to remain parked for two consecutive days on a continuous basis.

\subsection{Variables}
\label{sec:variables}

\subsubsection{Independent Variables}
\label{sec:independentvariables}

The independent variable is the testing technique. RT, RPT, DRT, and AT \cite{cai2004optimal} were used for comparison.

% table 5
\begin{table*}[hbtp]
  \caption{Plan C}
  \label{table:chinaA}
  \centering
  \begin{tabular}{lllllllllllll} \toprule
  \multicolumn{2}{l}{\multirow{2}{*}{Plan details}}                 &\multicolumn{11}{l}{Monthly charge~(CNY)} \\ \cmidrule(lr){3-13}
  \multicolumn{2}{l}{}                                                        &\!$op_C^1$\!  &\!$op_C^2$\!  &\!$op_C^3$\!  &\!$op_C^4$\!  &\!$op_C^5$\!  &\!$op_C^6$\! &\!$op_C^7$\!  &\!$op_C^8$\!  &\!$op_C^9$\!  &\!$op_C^{10}$\!  &\!$op_C^{11}$\! \\ \midrule
  \multirow{3}{*}{Basic} &Free calls~(min)  &50  &50  &240  &320  &420  &510  &700 &900 &1250 &1950  &3000 \\
                                            &Free data~(MB)  &150 &300 &300  &400  &500  &650  &750 &950 &1300 &2000  &3000 \\
                                            &Free incoming calls  &\multicolumn{11}{c}{Domestic~(including video calls)} \\ \midrule
  \multirow{3}{*}{Extra} &Incoming calls~(CNY/min)  &0.25 &0.20  &0.15 &0.15&0.15&0.15&0.15&0.15&0.15&0.15&0.15 \\
                                           &Data~(CNY/KB)  &3E-4&3E-4&3E-4&3E-4&3E-4&3E-4&3E-4&3E-4&3E-4&3E-4&3E-4 \\
                                            &Video calls~(CNY/min)&0.60&0.60&0.60&0.60&0.60&0.60&0.60&0.60&0.60&0.60&0.60\\\bottomrule
  \end{tabular}
\end{table*}

% table 6
\begin{table*}
  \caption{Hourly Parking Rates}
  \label{tab:hourlyRate}
  \centering
  \begin{tabular}{lllllll} \toprule
  \multirow{2}{*}{Actual parking hours} &\multicolumn{3}{l}{Weekday}&\multicolumn{3}{l}{Saturday and sunday} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}
                                  &Motorcycle  &Car: 2-door coupe  &Car: others    &Motorcycle  &Car: 2-door coupe  &Car: others \\ \midrule
  $(0.0,2.0]$                     &\$4.00        &\$4.50          &\$5.00      &\$5.00        &\$6.00        &\$7.00 \\
  $(2.0,4.0]$                     &\$5.00        &\$5.50          &\$6.00      &\$6.50      &\$7.50        &\$8.50  \\
  $(4.0,24.0]$                    &\$6.00        &\$6.50          &\$7.00      &\$8.00      &\$9.00        &\$10.00  \\ \bottomrule
  \end{tabular}
\end{table*}

\subsubsection{Dependent Variables}
\label{sec:dependentvariables}

The dependent variable for RQ1 is the metric for evaluating the fault-detection effectiveness.
Several effectiveness metrics exist, including:
the P-measure~\cite{duran1984evaluation} (the probability of at least one fault being detected by a test suite);
the E-measure~\cite{chen1997optimal} (the expected number of faults detected by a test suite);
the F-measure~\cite{sun2018adaptive} (the expected number of test case executions required to detect the first fault); and
the T-measure~\cite{zhang2014history} (the expected number of test cases required to detect all faults).
Since the F- and T-measures have been widely used for evaluating the fault-detection efficiency and effectiveness of DRT-related testing techniques~\cite{Cai07, cai2009random, Lv2011, Yang2014Dynamic, li2015, zhang2014history}, they are also adopted in this study.
We use $F$ and $T$ to represent the F-measure and the T-measure of a testing method.
As shown in Algorithm~\ref{alg:DRT}, the testing process may not terminate after the detection of the first fault.
Furthermore, because the fault detection information can lead to different probability profile adjustment mechanisms, it is also important to see what would happen after revealing the first fault.
Therefore, we introduce the F2-measure~\cite{sun2018adaptive} as the number of additional test cases required to reveal the second fault after detection of the first fault.
We use $F2$ to represent the F2-measure of a testing method, and $SD_{measure}$ to represent the standard deviation of metrics (where $measure$ can be $F$, $F2$, or $T$).

An obvious metric for RQ3 is the time required to detect faults.
Corresponding to the T-measure, in this study we used $T$-$time$, the time required to detect all faults.
$F$-$time$ and $F2$-$time$ denote the time required to detect the first fault, and the additional time needed to detect the second fault (after detecting the first), respectively.
For each of these metrics, smaller values indicate a better performance.

\subsection{Experimental Settings}
\label{sec:settings}

\subsubsection{Partitioning}
\label{sec:partition}

In our study, we set the partitions by making use of a decision table (DT) \cite{gettys1986if}.
A DT presents a large amount of complex decisions in a simple, straightforward manner, representing a set of decision rules under all exclusive conditional scenarios in a pre-defined problem.
Typically, a DT consists of four parts:

\begin{enumerate}
	\item
	The upper-left part lists the conditions denoted $C_i$ ($i = 1, \ldots, n$, where $n$ is the number of conditions in the pre-defined problem,
    and $n \ge 1$).
	Each condition $C_i$ contains a set of possible options $O_{i,q} \in CO_i = \{O_{i,1}, \ldots, O_{i,t_{i}}\}$, where $t_i$ is the number of
    possible options for $C_i$, and $q = \{1,\ldots,t_i\}$.
	
	\item
	The upper-right part shows the condition space, which is a Cartesian product of all the $CO_i$	($SP(C) = CO_1 \times CO_2 \times \ldots \times CO_n$).
	Each element in the $SP(C)$ is a condition entry ($CE$) with the ordered $n$-tuple.
	
	\item
	The lower-left part shows all possible actions, represented $A_j$ ($j = 1, \ldots, m$, where $m$ is the number of possible actions and $m \ge 1$).
	Similar to $CO_i$, an action $A_j$ contains a set of possible options $O_{j,p}^{'} \in AO_j = \{O_{j,1}^{'}, \ldots, O_{j,k_{j}}^{'}\}$, where $k_j$ is the number of alternatives for $A_j$, and $p = \{1,\ldots,k_j\}$.

	\item
	The lower-right part shows the action space $SP(A)$, which is also a Cartesian product of all the $AO_j$ ($SP(A) = AO_1 \times AO_2 \times \ldots \times AO_m$).
	Similar to \emph{CE}, each element in the $SP(A)$ is an action entry (\emph{AE}) with the ordered $m$-tuple.
	
\end{enumerate}

A DT \emph{rule} is composed of a \emph{CE} and its corresponding \emph{AE}.
With DT, it is possible to obtain partition schemes with different granularities.
For fine-grain partition schemes, each \emph{CE} of a DT \emph{rule} corresponds to a partition;
while for coarse-grained schemes, a partition corresponds to the union of a group of partitions for which all \emph{CE} of DT \emph{rules} have the same \emph{AE}.
The decision tables for \texttt{ACMS}, \texttt{CUBS}, and \texttt{PBS} are shown in Tables \ref{tab:ds-acms} to \ref{tab:ds-pbs}, respectively. In the tables, $R_i$ $(i = 1, 2, \ldots, n)$ denotes the identified $i^{th}$ \emph{rule}; $n$ is the total number of \emph{rules}; and the checkmark (\checkmark) under each \emph{rule} indicates that the corresponding action should be taken.
The details of actions are provided in Table \ref{tab:formulas}, where $w$ is the weight of baggage; $price$ means economy class fare; $op$ means the monthly charge; $call$ and $data$ mean the call duration and data usage, respectively; $freeCall$ and $freeData$ mean the free calls and free data, respectively; and $baseFee$ means the cost before the discount. In Table \ref{tab:ds-acms}, the condition for calculating the cost of the baggage includes \emph{class} (0: first class; 1: business class; and 2: economy class), \emph{isStudent} (Y: the passenger is a student; and N: the passenger is not a student), \emph{isOverload} (Y: the baggage exceeds the free carry-on weight limit; and N: the baggage does not exceed the free carry-on weight limit.), and \emph{Destination} (0: domestic flight; and 1: international flight). In Table \ref{tab:ds-cubs}, conditions that influence cell-phone bills include \emph{plan} (A: plan A; B: plan B; and C: plan C) and \emph{option} $y$ under \emph{plan} $x$, represented as $op_x^y$, where $x \in \{A, B, C\}$, and $y \in \{w|1 \le w \le 11 \wedge w \in \mathcal{Z}\} $. In Table \ref{tab:ds-pbs}, conditions that affect the parking fee include the type of \emph{vehicle} (0: motorcycle; 1: 2-door coupe; and 2: others), \emph{day of week} (0: weekday; and 1: saturday or sunday), and \emph{discount information} (0: customers provide a discount coupon; 1: the estimated hours of parking and the actual hours of parking fall into the same time range; and 2: estimated hours and the actual hours are in different time ranges).

% table 7
\begin{table*}
  \caption{Decision Table for \texttt{ACMS}}
  \label{tab:ds-acms}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lllllllllllllllllllllllll}
  \toprule
      &\!$R_1$\! &\!$R_2$\! &\!$R_3$\! &\!$R_4$\! &\!$R_5$\! &\!$R_6$\! &\!$R_7$\! &\!$R_8$\! &\!$R_9$\! &\!$R_{10}$\! &\!$R_{11}$\! &\!$R_{12}$\! &\!$R_{13}$\! &\!$R_{14}$\! &\!$R_{15}$\! &\!$R_{16}$\! &\!$R_{17}$\! &\!$R_{18}$\! &\!$R_{19}$\! &\!$R_{20}$\! &\!$R_{21}$\! &\!$R_{22}$\! &\!$R_{23}$\! &\!$R_{24}$\! \\
  \midrule
  class &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 \\
  destination &0 &0 &0 &1 &1 &1 &0 &0 &0 &1 &1 &1 &0 &0 &0 &1 &1 &1 &0 &0 & 0 &1 &1 &1 \\
  isStudent &N &N &N &N &N &N &Y &Y &Y &Y &Y &Y &N &N &N &N &N &N &Y &Y &Y &Y &Y &Y \\
  isOverload &N &N &N &N &N &N &N &N &N &N &N &N &Y &Y &Y &Y &Y &Y &Y &Y &Y &Y &Y &Y \\
  \midrule
  $f_{1,1}$ &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark & & & & & & & & & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{1,2}$ & & & & & & & & & & & & &\checkmark & & & & & &\checkmark & & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{1,3}$ & & & & & & & & & & & & & &\checkmark & & & & & &\checkmark & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{1,4}$ & & & & & & & & & & & & & & &\checkmark & & & & & &\checkmark & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{1,5}$ & & & & & & & & & & & & & & & &\checkmark & & & & & &\checkmark & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{1,6}$ & & & & & & & & & & & & & & & & &\checkmark & & & & & &\checkmark &\checkmark \\ \specialrule{0em}{1pt}{1pt}
  $f_{1,7}$ & & & & & & & & & & & & & & & & & &\checkmark & & & & & & \\
  \bottomrule
  \end{tabular}}
\end{table*}

%table 8
\begin{table*}
  \caption{Decision Table for \texttt{CUBS}}
  \label{tab:ds-cubs}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lllllllllllllllllllll}
  \toprule
      &\!$R_1$\! &\!$R_2$\! &\!$R_3$\! &\!$R_4$\! &\!$R_5$\! &\!$R_6$\! &\!$R_7$\! &\!$R_8$\! &\!$R_9$\! &\!$R_{10}$\! &\!$R_{11}$\! &\!$R_{12}$\! &\!$R_{13}$\! &\!$R_{14}$\! &\!$R_{15}$\! &\!$R_{16}$\! &\!$R_{17}$\! &\!$R_{18}$\! &\!$R_{19}$\! &\!$R_{20}$\! \\
  \midrule
  plan &A &A &A &B &B &B &B &B &B &C &C &C &C &C &C &C &C &C &C &C \\
  option &$op_A^1$ &$op_A^2$ &$op_A^3$ &$op_B^1$ &$op_B^2$ &$op_B^3$ &$op_B^4$ &$op_B^5$ &$op_B^6$ &$op_C^1$ &$op_C^2$ &$op_C^3$ &$op_C^4$ &$op_C^5$ &$op_C^6$ &$op_C^7$ &$op_C^8$ &$op_C^9$ &$op_C^{10}$ &$op_C^{11}$ \\
  \midrule
  $f_{2,1}$ &\checkmark &\checkmark &\checkmark & & & & & & & & & & & & & & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{2,2}$ & & & &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark & & & & & & & & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{2,3}$ & & & & & & & & & &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark \\ \specialrule{0em}{1pt}{1pt}
  \bottomrule
  \end{tabular}}
\end{table*}

% table 9
\begin{table*}[!htp]
  \caption{Decision Table for \texttt{PBS}}
  \label{tab:ds-pbs}
  \centering
  \begin{tabular}{lllllllllllllllllll}
  \toprule
      &\!$R_1$\! &\!$R_2$\! &\!$R_3$\! &\!$R_4$\! &\!$R_5$\! &\!$R_6$\! &\!$R_7$\! &\!$R_8$\! &\!$R_9$\! &\!$R_{10}$\! &\!$R_{11}$\! &\!$R_{12}$\! &\!$R_{13}$\! &\!$R_{14}$\! &\!$R_{15}$\! &\!$R_{16}$\! &\!$R_{17}$\! &\!$R_{18}$\!  \\
  \midrule
  vehicle        &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 &0 &1 &2 \\
  time           &0 &0 &0 &1 &1 &1 &0 &0 &0 &1 &1 &1 &0 &0 &0 &1 &1 &1 \\
  discount       &0 &0 &0 &0 &0 &0 &1 &1 &1 &1 &1 &1 &2 &2 &2 &2 &2 &2 \\
  \midrule
  $f_{3,1}$    &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark & & & & & & & & & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{3,2}$    & & & & & & &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark & & & & & & \\ \specialrule{0em}{1pt}{1pt}
  $f_{3,3}$    & & & & & & & & & & & & &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark \\ \specialrule{0em}{1pt}{1pt}
  \bottomrule
  \end{tabular}
\end{table*}

% table 10
\begin{table}
  \caption{Formulas of the Actions in Table \ref{tab:ds-acms} $\sim$ \ref{tab:ds-pbs}}
  \label{tab:formulas}
  \centering
  \begin{tabular}{ll}
  \toprule
    Web Service           & Formulas       \\ \midrule
                          & $f_{1,1} = 0$ \\ \specialrule{0em}{1px}{1px}
                          & $f_{1,2} = (w -25) \times  price \times 1.5\%$   \\\specialrule{0em}{1px}{1px}
                          & $f_{1,3} = (w - 35) \times price \times 1.5\%$             \\\specialrule{0em}{1px}{1px}
    \texttt{ACMS}         & $f_{1,4} = (w - 25) \times price \times 1.5\%$             \\\specialrule{0em}{1px}{1px}
                          & $f_{1,5} = (w - 47) \times price \times 1.5\%$ \\\specialrule{0em}{1px}{1px}
                          & $f_{1,6} = (w - 37) \times price \times 1.5\%$ \\\specialrule{0em}{1px}{1px}
                          & $f_{1,7} = (w - 27) \times price \times 1.5\%$ \\ \midrule
                          & $f_{2,1} = op + (call - freeCall) \times 0.25$ \\
                          &            $\qquad \quad +\;(data - freeData) \times 0.0003$ \\\specialrule{0em}{1px}{1px}
   \texttt{CUBS}          & $f_{2,2} = op + (call - freeCall) \times 0.20$ \\
                          &            $\qquad \quad +\;(data - freeData) \times 0.0003$ \\ \specialrule{0em}{1px}{1px}
                          & $f_{2,3} = op + (call - freeCall) \times 0.15$ \\
                          &            $\qquad \quad +\;(data - freeData) \times 0.0003$ \\ \midrule
                          & $f_{3,1} = baseFee \times 50\%$ \\ \specialrule{0em}{1px}{1px}
    \texttt{CUBS}         & $f_{3,2} = baseFee \times (1-40\%)$ \\ \specialrule{0em}{1px}{1px}
                          & $f_{3,3} = baseFee \times (1 +20\%)$ \\
   \bottomrule
  \end{tabular}

\end{table}



As can be seen from the description above, because the DT considers all parameters, and identifies their invalid combinations, it can  provide a systematic and efficient way to partition an input domain into disjoint subdomains, and then generate test cases. In practice, each DT \emph{rule} condition entry corresponds to a partition in which test cases cover some paths
---
thus, the faults in those paths have a chance of being detected.

% table 11
\begin{table}
  \caption{Initial Test Profile for Subject Web Services}
  \label{tab:initialtf}
  \centering
  \renewcommand\tabcolsep{2.5pt}
  \begin{tabular}{lll} \toprule
  Actual parking                 &Hourly parking    & Initial test   \\
  hours                          &rates             & profile         \\ \midrule
  \multirow{2}{*}{\texttt{ACMS}}      &24   &$\{\left \langle s_1, \frac{1}{24} \right \rangle, \left \langle s_2, \frac{1}{24} \right \rangle,\ldots, \left \langle s_{24}, \frac{1}{24} \right \rangle\}$  \\ \specialrule{0em}{1px}{1px}
                             &7   &$\{\left \langle s_1, \frac{1}{7} \right \rangle, \left \langle s_2, \frac{1}{7} \right \rangle, \ldots,\left \langle s_7, \frac{1}{7} \right \rangle\}$  \\ \midrule
  \multirow{2}{*}{\texttt{CUBS}}      &20 &$\{\left \langle s_1, \frac{1}{20} \right \rangle, \left \langle s_2, \frac{1}{20} \right \rangle,\ldots, \left \langle s_{20}, \frac{1}{20} \right \rangle\}$  \\ \specialrule{0em}{1px}{1px}
                             &3  &$\{\left \langle s_1, \frac{1}{3} \right \rangle, \left \langle s_2, \frac{1}{3} \right \rangle, \ldots, \left \langle s_3, \frac{1}{3} \right \rangle\}$  \\ \midrule
  \multirow{2}{*}{\texttt{PBS}}       &18  &$\{\left \langle s_1, \frac{1}{18} \right \rangle, \left \langle s_2, \frac{1}{18} \right \rangle,\ldots, \left \langle s_{18}, \frac{1}{18} \right \rangle\}$  \\ \specialrule{0em}{1px}{1px}
                             &3                     &$\{\left \langle s_1, \frac{1}{3} \right \rangle, \left \langle s_2, \frac{1}{3} \right \rangle, \ldots,\left \langle s_3, \frac{1}{3} \right \rangle\}$  \\ \bottomrule
  \end{tabular}
\end{table}

%% table 12
%\begin{table*}[htbp]
%  \caption{Number of Scenarios where the Technique on the top row has a lower metric (F-/F2-/T-measure) score than the technique on the left column}
%  \centering
%  \label{tableHlom:f/f2/t-measure}
%  \begin{tabular}{lllllllllllll}  \toprule
%  \multirow{2}{*}{}&\multicolumn{4}{l}{F-measure}&\multicolumn{4}{l}{F2-measure}&\multicolumn{4}{l}{T-measure}\\ \cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
%     &\!RT\! &\!RPT\! &\!AT\! &\!DRT\!           &\!RT\!&\!RPT\!&\!AT\!&\!DRT\!                 &\!RT\!&\!RPT\!&\!AT\!&\!DRT\!\\ \midrule
%  RT &---    &4       &5   &5    &---  &4      &5     &6   &---   &6     &6    &6 \\ \specialrule{0em}{1px}{1px}
%  RPT&2      &---     &6   &6    &2    &---    &5     &6   &0     &---   &6    &6 \\ \specialrule{0em}{1px}{1px}
%  AT &1      &0       &--- &4    &1    &1      &---   &6   &0     &0     &---  &5 \\ \specialrule{0em}{1px}{1px}
%  DRT&1      &0       &2   &---  &0    &0      &0     &--- &0     &0     &1    &---  \\ \bottomrule
%  \end{tabular}
%\end{table*}

\subsubsection{Initial Test Profile}
\label{sec:profile}

Because test cases may be generated randomly during the test process, a feasible method is to use a uniform probability distribution as the initial testing profile.
On the other hand, testers may also use past experience to help guide selection of a different probability distribution as the initial profile. In our experiment, we used a uniform probability distribution for the initial test profile. The initial test
profiles of each web service are summarized in Table \ref{tab:initialtf}, where $<s_i, p_i>$
means that the probability of selecting partition $s_i$ is $p_i$.

\subsubsection{Constants}
\label{sec:constant}

In the experiments, we were interested in exploring the relationship between the number of partitions and the DRT strategy parameter $\varepsilon$, and therefore selected a set of parameter values:
$\varepsilon \in \{1.0E$-$05, 5.0E$-$05, 1.0E$-$04, 5.0E$-$04, 1.0E$-$03, 5.0E$-$03, 1.0E$-$02, 5.0E$-$02, 1.0E$-$01, 2E$-$01, 3E$-$01, 4E$-$01, 5E$-$01\}$.
It should be noted that $\varepsilon = 5E$-$01$ is already a large value.
Consider the following scenario.
For PBS, when the test is carried out under partition scheme 2, if $\varepsilon = 7.5E$-$01$ and  a uniform probability distribution is used as the testing profile (that is, $p_i = 1/3$) , then suppose that the first test case belonging to $c_1$ is executed and does not reveal any faults, then, according to Formula 3, the value of $p_1$ would become $0$.
It is important, therefore, that the initial value of $\varepsilon$ should not be set too large.

\subsection{Experimental Environment}
\label{sec:environment}

Our experiments were conducted on a virtual machine running the Ubuntu 11.06 64-bit operating system, with two CPUs, and a memory of 2GB.
The test scripts were written in Java.
To ensure statistically reliable values \cite{arcuri2011practical} of the metrics (F-measure, F2-measure, T-measure, F-time, F2-time, and T-time), each testing session was repeated 30 times with 30 different seeds, and the average value calculated.

\subsection{Threats To Validity}
\label{sec:threats}

\subsubsection{Internal Validity}
\label{sec:internalthreats}

A threat to internal validity is related to the implementations of the testing techniques, which involved a moderate amount of programming work.
However, our code was cross-checked by different individuals, and we are confident that all techniques were correctly implemented.

\subsubsection{External Validity}
\label{sec:externalthreats}

The possible threat to external validity is related to the subject programs and seeded faults under evaluation.
Although the three subject web services are not very complex, they do implement real-life business scenarios of diverse application domains.
Furthermore, 18 distinct faults were used to evaluate the performance. These faults cover different types of mutation operators and require an average of more than 20 randomly generated test cases to be detected. Although we have tried to improve the generalisability of the findings by applying different partitioning granularities, and 13 kinds of parameters, we anticipate that the evaluation results may vary slightly with different subject web services.

\subsubsection{Construct Validity}
\label{sec:construct}

The metrics used in our study are simple in concept and straightforward to apply, and hence there should be little threat to the construct validity.

\subsubsection{Conclusion Validity}
\label{sec:conclusion}

As reported for empirical studies in the field of software engineering \cite{arcuri2011practical},  at least 30 observations are necessary to ensure the statistical significance of results.
Accordingly, we have run a sufficient number of trials to ensure the  reliability of our experimental results.
Furthermore, as will be discussed in Section~\ref{sec:results}, we also conducted statistical tests to confirm the significance of the results.

\section{Experimental Results}
\label{sec:results}

\subsection{RQ1: Fault Detection Effectiveness}
\label{sec:RQ1}

F-, F2-, and T-measure results for \texttt{ACMS}, \texttt{CUBS}, and \texttt{PBS} are shown using boxplots in
Figures~\ref{fig:Fmeasure} to~\ref{fig:Tmeasure}, where the DRT parameter $\varepsilon$ was set to the optimal values, as described in Section \ref{sec:RQ2}. The experimental results of DRT with other values of $\varepsilon$ are shown in Appendix B.
In each boxplot, the upper and lower bounds of the box represent the third and first quartiles of the metric, respectively;
the middle line represents the median value;
the upper and lower whiskers mark, respectively, the largest and smallest data within the range of $\pm 1.5 \times IQR$ (where $IQR$ is the interquartile range);
outliers beyond the  $IQR$ are denoted with hollow circles; and
each solid circle represents the mean value of the metric.

%fig 3
\begin{figure}
	\centering
    \subfigure[\texttt{ACMS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/acmsFbySun.pdf}}
	\subfigure[\texttt{CUBS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/cubsFbySun.pdf}}
	\subfigure[\texttt{PBS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/pbsFbySun.pdf}}
	\caption{F-measure boxplots for each web service}
	\label{fig:Fmeasure}
\end{figure}

%fig 4
\begin{figure}
	\centering
	\subfigure[\texttt{ACMS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/acmsF2bySun.pdf}}
	\subfigure[\texttt{CUBS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/cubsF2bySun.pdf}}
	\subfigure[\texttt{PBS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/pbsF2bySun.pdf}}
	\caption{F2-measure boxplots for each web service}
	\label{fig:F2measure}
\end{figure}

%fig 5
\begin{figure}
	\centering
	\subfigure[\texttt{ACMS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/acmsTbySun.pdf}}
	\subfigure[\texttt{CUBS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/cubsTbySun.pdf}}
	\subfigure[\texttt{PBS}] {\includegraphics[width=0.48\textwidth,height=7cm]{fig/drtresultbox/pbsTbySun.pdf}}
	\caption{T-measure boxplots for each web service}
	\label{fig:Tmeasure}
\end{figure}

It can observed from the figures that, in an overwhelming majority of cases, DRT was the best performer in terms of F-, F2-, and T-measure, followed by AT, RPT, and RT. On the other hand, RT may be the best performer occasionally or the worst performer in terms of F-, F2-, and T-measure, which means that the fault detection effectiveness of RT is not stable. In contrast, DRT and AT show a relatively stable fault detection effectiveness.
We also conducted statistical testing to verify the significance of this observation, using the Holm-Bonferroni method~\cite{sun2018adaptive} (with p-value equal to $0.05$) to determine which pairs of testing techniques had significant differences.
The statistical data are shown in Table \ref{tableHlom:f/f2/t-measure}, where each cell gives the number of scenarios where the technique above (in the table) performed better than one to the left.
For example, the ``6'' in the top right cell of Table~\ref{tableHlom:f/f2/t-measure} indicates that, of 6 scenarios (two partition schemes $\times$ three web services), DRT had lower T-measure scores than RT for 6, with the fault-detection capabilities of these two techniques being significantly different.

Table~\ref{tableHlom:f/f2/t-measure} clearly shows that the differences between pairs of testing techniques are all significant.
% table 12
\begin{table}[htbp]
  \caption{Number of Scenarios where the Technique on the top row has a lower metric (F-/F2-/T-measure) score than the technique on the left column}
  \centering
  \label{tableHlom:f/f2/t-measure}
  \renewcommand\tabcolsep{4.0pt}
  \begin{tabular}{lllllllllllll}  \toprule
  \multirow{2}{*}{}&\multicolumn{4}{l}{F-measure}&\multicolumn{4}{l}{F2-measure}&\multicolumn{4}{l}{T-measure}\\ \cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
     &\!RT\! &\!RPT\! &\!AT\! &\!DRT\!           &\!RT\!&\!RPT\!&\!AT\!&\!DRT\!                 &\!RT\!&\!RPT\!&\!AT\!&\!DRT\!\\ \midrule
  RT &---    &4       &5   &5    &---  &4      &5     &6   &---   &6     &6    &6 \\ \specialrule{0em}{1px}{1px}
  RPT&2      &---     &6   &6    &2    &---    &5     &6   &0     &---   &6    &6 \\ \specialrule{0em}{1px}{1px}
  AT &1      &0       &--- &4    &1    &1      &---   &6   &0     &0     &---  &5 \\ \specialrule{0em}{1px}{1px}
  DRT&1      &0       &2   &---  &0    &0      &0     &--- &0     &0     &1    &---  \\ \bottomrule
  \end{tabular}
\end{table}

\subsection{RQ2: Relationship between Partition Number and $\varepsilon$}
\label{sec:RQ2}

In Section \ref{sec:relation}, we analyzed the relationship between the number of partitions and the DRT parameter $\varepsilon$.
In this section, we show that our theoretical analysis provides useful guidance to testers to set the value of $\varepsilon$.

We used three web services to validate our theoretical analysis.
Before starting the test, it is necessary to know the failure rate $\theta_i$ of partition $s_i$. From Tables \ref{tab:aviation} to \ref{table:chinaA}, it can be observed that the values of some parameters (such as the baggage weight, the call duration, and parking duration) are such that the total number of test case values in a partition could be infinite. For such a situation, we approximate the failure rate $\theta_i$ of $s_i$ by $1 / k_i$ (where $k_i$ is the total number of test cases executed before revealing a fault).
According to Formula 19, the theoretically optimal values of~$\varepsilon$ in each scenario for each web service are shown in Table~\ref{tab:parameters}, where $\varepsilon^{*}$ denotes the theoretical value of $\varepsilon$.
We ran a series of experiments with the parameters set according to those in Table~\ref{tab:parameters}:
The F-, F2-, and T-measure results for each  program are shown in Figure~\ref{fig:theovsnontheo}, where $\varepsilon_1^{*}$ and $\varepsilon_2^{*}$ denote the theoretical values of parameter $\varepsilon$ in the two different partition schemes, respectively.
For ease of presentation and understanding, we used $log_{100}(1.0E05 \times \varepsilon)$ for the horizontal axis in Figure \ref{fig:theovsnontheo}.
Apart from the DRT strategy parameter $\varepsilon$, all other experimental settings remained the same as in Section~\ref{sec:RQ1}.


%fig 6
\begin{figure*}[htbp]
	\centering
    \subfigure[\texttt{ACMS} Partition Scheme 1] {\includegraphics[width=0.49\textwidth,height=5cm]{fig/drtparemeter/avias1.pdf}}
    \subfigure[\texttt{ACMS} Partition Scheme 2] {\includegraphics[width=0.49\textwidth,height=5cm]{fig/drtparemeter/avias2.pdf}}
    \subfigure[\texttt{CUBS} Partition Scheme 1] {\includegraphics[width=0.49\textwidth,height=5cm]{fig/drtparemeter/cubss1.pdf}}
    \subfigure[\texttt{CUBS} Partition Scheme 2] {\includegraphics[width=0.49\textwidth,height=5cm]{fig/drtparemeter/cubss2.pdf}}
    \subfigure[\texttt{PBS} Partition Scheme 1] {\includegraphics[width=0.49\textwidth,height=5cm]{fig/drtparemeter/pbss1.pdf}}
    \subfigure[\texttt{PBS} Partition Scheme 2] {\includegraphics[width=0.49\textwidth,height=5cm]{fig/drtparemeter/pbss2.pdf}}
	\caption{Line charts of F-measure, F2-measure, and T-measure values for each web service (for both the theoretically optimum parameter value, and other values}
	\label{fig:theovsnontheo}
\end{figure*}

From Figure~\ref{fig:theovsnontheo}, we have the following observations:
\begin{itemize}
  \item
  In most scenarios, the DRT strategy with theoretically optimum parameter value performs best.
  Furthermore, the DRT strategy performs better when the parameter values are near the theoretically optimum value than when not.

  \item
  From Figure \ref{fig:theovsnontheo} (a), it can be observed that the DRT strategy with larger parameter values performs better than with the theoretically optimum value, in terms of the F-measure.
  The main reason for this is that, for this scenario, the maximum failure rate ($\theta_M = 4.781E-3$) is large and the number of partitions is small:
  When the parameter value is large, the probability of selecting partitions with lower failure rates is quickly reduced, and the probability of selecting partitions with larger failure rates is quickly increased, according to Formulas 3 and 4.
\end{itemize}

% table 13
\begin{table}
  \caption{Theoretical Optimal Values of DRT Parameter}
  \centering
  \label{tab:parameters}
  \begin{tabular}{llll} \toprule
     Web                         & Partition  &\multirow{2}{*}{$\theta_{min}$}   &\multirow{2}{*}{$\varepsilon^{*}$}    \\
     service                     & scheme     &                 &                \\ \midrule
     \multirow{2}{*}{\texttt{ACMS}}       &1           &5.452E-2         &1.601E-1        \\
                                 &2           &2.797E-3         &1.102E-4        \\ \midrule
     \multirow{2}{*}{\texttt{CUBS}}       &1           &1.193E-3         &5.702E-5        \\
                                 &2           &1.397E-3         &1.734E-5        \\ \midrule
     \multirow{2}{*}{\texttt{PBS}}        &1           &1.760E-3         &1.118E-4        \\ \
                                 &2           &1.492E-3         &1.340E-5        \\ \bottomrule
  \end{tabular}
\end{table}

\subsection{RQ3: Fault Detection Efficiency}
\label{sec:RQ3}

The F-, F2-, and T-time results for \texttt{ACMS}, \texttt{CUBS}, and \texttt{PBS} are summarized in Table \ref{tab:time}, where the values of DRT parameters for the subject web services are the same as those in Section \ref{sec:RQ1}. The F-, F2-, and T-time results of DRT with different parameter values are summarized in Appendix B. It can be observed from the table that, in general, DRT had the best performance; RPT marginally outperforms RT; and AT had the worst performance.

As was done for the F-, F2-, and T-measure data, we used the Holm-Bonferroni method to check the difference between each pair of testing strategies in terms of F-time, F2-time, and T-time, as shown in Table \ref{tableHlom:f/f2/t-time}.
Table \ref{tableHlom:f/f2/t-time} shows that: a) DRT was significantly better than AT in terms of F-/F2-/T-time; b) DRT was significantly better than RT and RPT in terms of F2-/T-time; and c) DRT marginally outperformed RT and RPT in terms of F-time.
In other words, the additional computation incurred in DRT by updating the test profile is compensated for in terms of test execution savings.

In summary, the DRT strategy is considered the best testing technique across this three metrics, RPT marginally outperformed RT, and DRT, RPT, and RT significantly outperformed AT.

% table 14
\begin{table*}
	\caption{F-time, F2-time, and T-time in ms for Subject Web Services}
	\centering
	\label{tab:time}
	\begin{tabular}{llllllllllllll}
	\toprule
	Partition&\multirow{2}{*}{Metric}&\multicolumn{4}{l}{\texttt{ACMS}}&\multicolumn{4}{l}{\texttt{CUBS}}&\multicolumn{4}{l}{\texttt{PBS}} \\ \cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14}
	Scheme   &        &RT  &RPT   &AT     &DRT    &RT   &RPT   &AT      &DRT   &RT  &RPT   &AT    &DRT  \\ \midrule
	         &F-time  &0.43&0.57  &0.49   &0.23   &0.82 &0.91  &140.13  &0.95  &0.81&0.85  &22.25 &0.68 \\
	1        &F2-time &0.29&0.31  &2.47   &0.12   &1.14 &0.87  &172.27  &0.86  &0.42&0.52  &25.40 &0.34 \\
	         &T-time  &0.85&1.08  &2.47   &0.43   &34.69&30.54 &32429.07&30.21 &4.12&3.83  &289.04&3.20 \\ \midrule
	         &F-time  &0.43&0.33  &15.53  &0.24   &0.82 &0.75  &16.82   &0.87  &0.81&0.66  &12.99 &0.49 \\
	2        &F2-time &0.29&0.45  &363.47 &0.28   &1.14 &0.79  &15.76   &0.83  &0.42&0.35  &17.44 &0.34 \\
	         &T-time  &0.85&0.78  &459.67 &0.65   &34.69&34.59 &2666.17 &36.49 &4.12&2.98  &200.54&2.26 \\
	\bottomrule
	\end{tabular}
\end{table*}

% table 15
\begin{table}
  \caption{Number of Scenarios where the Technique on the top row has a lower metric (F-/F2-/T-time) score than the technique on the left column}
  \centering
  \label{tableHlom:f/f2/t-time}
  \renewcommand\tabcolsep{4.0pt}
  \begin{tabular}{lllllllllllll}  \toprule
  \multirow{2}{*}{}&\multicolumn{4}{l}{F-time}&\multicolumn{4}{l}{F2-time}&\multicolumn{4}{l}{T-time}\\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
  \cmidrule(lr){10-13}
     &\!RT\! &\!RPT\! &\!AT\!&\!DRT\!      &\!RT\!&\!RPT\!&\!AT\!&\!DRT\!              &\!RT\!&\!RPT\!&\!AT\!&\!DRT\!\\\midrule
  RT & ---   &3       &0     &4     &---   &3      &0   &6   &--- &5     &0     &6  \\
  RPT&3      & ---    &1     &4     &3     & ---   &0   &5   &1   &---   &0     &6 \\
  AT &6      &5       &---   &6     &6     &6      &--- &6   &6   &6     &---   &6    \\
  DRT&2      &2       &0     &---   &0     &1      &0   &--- &0   &0     &0     & ---  \\ \bottomrule
  \end{tabular}
\end{table}


\subsection{Summary}
\label{sec:summary}

Based on the evaluation, we have the following observations:
\begin{itemize}
  \item
  DRT outperformed RT, RPT, and AT, according to all the applied metrics for all three studied web services. DRT marginally outperformed AT in terms of the F-, F2-, and T-measure, for all the studied web services. Moreover, AT incurs heavier computational overhead, and takes a significantly longer time. For instance, AT required 32429.07ms to select and execute sufficient test cases to detect all faults in \texttt{CUBS}, while DRT only needed 30.21ms (Table \ref{tab:time}). This indicates that among RT, RPT, and AT, DRT should be chosen.

  \item
  DRT is more effective in terms of the F-, F2-, and T-measure when the parameter settings are optimal (according to the theoretical analysis): In most cases, DRT has the best performance for all three web services, according to these three metrics (F-measure, F2-measure, and T-measure) when following the guidelines for the parameter settings. This highlights the usefulness of the parameter-setting guidelines.
\end{itemize}

We also note the following limitations:

\begin{itemize}
  \item
  While DRT outperformed RT and RPT in terms of fault detection effectiveness and efficiency, this was achieved at the cost of the additional effort required to set the partitions and test profiles.

  \item
  Applying DRT involves setting parameters, which may not be trivial. Even when following the theoretical guidelines.
\end{itemize}


\section{Related Work}
\label{sec:relatedwork}

In this section, we describe related work from two perspectives:
related to testing techniques for web services; and
related to improving RT and PT.

\subsection{Testing Techniques for Web Services}
\label{sec:relatedworkWS}

In recent years, a lot of effort has been made to test web services~\cite{canfora, bozkurt2010, li2014two, qiu2015regression}.
Test case generation or selection is core to testing web services, and model-based~\cite{dalal1999model} and specification-based~\cite{li2009towards} techniques are two common approaches.
Before making services available on the Internet, testers can use model-based techniques to verify whether or not the behavior of the WSUT meets their requirements.
In these techniques, test data can be generated from a data model that specifies the inputs to the software---this data model can be built before, or in parallel to, the software development process.
Verification methods using technologies such as theorem-proving~\cite{sinha2006model}, models~\cite{paradkar2007} and Petri-Nets~\cite{xiang2015executable} also exist.

All of the above approaches aim to generate test cases without considering the impact of test case execution order on test efficiency.
In contrast, Bertolino et al.~\cite{bertolino2007automatic} proposed using the category-partition method~\cite{Ostrand88} with XML schemas to perform XML-based partition testing.
Because PT aims to find subsets of all possible test cases to adequately test a system, it can help reduce the required number of test cases.
Our approach involves software cybernetics and PT:
In DRT, selection of a partition is done according to the testing profile, which is updated throughout the testing process.
An advantage of DRT is that partitions with larger failure rates have higher probabilities of selection.
Zhu and Zhang~\cite{zhu2012collaborative} proposed a collaborative testing framework, where test tasks are completed using collaborating test services---a test service is a service assigned to perform a specific testing task.
Our framework (Section~\ref{sec:framework}) aims to find more faults in the WSUT, with the result of the current test case execution providing feedback to the control system so that the next test case selected has a greater chance to reveal faults.

Most web service testing techniques assume that the computed output for any test case is verifiable, which is, however, not always true in practice (a situation known as the oracle problem \textcolor{blue}{[26][27]}\cite{barr2015oracle}). %\cite{barr2015oracle, patel2018mapping}). 
Thus, many testing techniques may not be applicable in some cases.
To address the oracle problem for testing web services, Sun et al. \cite{sun2011} proposed a metamorphic testing \cite{chen1998metamorphic, chen2018metamorphic} approach that not only alleviates the oracle problem, but is also a practical and efficient option for testing web services. They conducted a case study that showed that up to $94.1\%$ of seeded faults could be detected without the need for oracles.

\subsection{Improving RT and PT}
\label{sec:relatedworkRT}

Based on the observation that failure-causing inputs tend to cluster into contiguous regions in the input domain~\cite{Ammann88, Finelli91}, much work has been done to improve RT \cite{cai2009random, chen2010adaptive, 8846002}.
Adaptive random testing~\cite{chen2010adaptive, 8846002} is a family of techniques based on random testing that aim to improve the failure-detection effectiveness by evenly spreading test cases throughout the input domain.
One well-known ART approach, FSCS-ART, selects a next test input from the fixed-size candidate set of tests that is farthest from all previously executed tests~\cite{chen2004adaptive}.
Many other ART algorithms have also been proposed, including RRT
\textcolor{blue}{[57], [58]}
\cite{chan2006restricted},
 DF-FSCS \cite{mao2017out}, and ARTsum \cite{barus2016cost}, with their effectiveness examined and  validated through simulations and experiments.

Adaptive testing (AT) \cite{Cai07, hu2005case, hu2009improved} takes advantage of feedback information to control the execution process, and has been shown to outperform RT and RPT in terms of the T-measure and the number of detected faults, which means that AT has higher efficiency and effectiveness than RT and RPT.
However, AT may require a rather long execution time in practice.
To alleviate this, Cai et al.~\cite{cai2009random} proposed DRT, which uses testing information to dynamically adjust the testing profile.
There are several things that can impact on DRT's test efficiency.
Yang et al.~\cite{Yang2014Dynamic} proposed A-DRT, which adjusts parameters during the testing process.

\section{Conclusion}
\label{sec:conclusion}

In this paper, to address the challenges of testing SOA-based applications, we have presented a dynamic random testing (DRT) method for web services.
Our method uses random testing to generate test cases, and selects test cases from different partitions in accordance with a testing profile that is dynamically updated in response to the test data collected.
In this way, the proposed method enjoys benefits from both random testing and partition testing.

We proposed a framework that examines key issues when applying DRT to test web services, and developed a prototype to make the method practical and effective.
To guide testers to correctly set the DRT parameters, we used a theoretical analysis to study the relationships between the number of partitions ($m$) and the probability adjusting factor ($\varepsilon$).
Three real web services were used as experimental subjects to validate the feasibility and effectiveness of our approach.
Our experimental results show that, in general, DRT has better performance than both RT and RPT, in terms of the F-, F2-, and T-measures, and always outperforms when the $\varepsilon$ settings follow our guidelines.
In other words, our theoretical analysis can provide genuinely useful guidance to use DRT.

In our future work, we plan to conduct experiments on more web services to further validate the effectiveness, and identify the limitations of our method.

\section*{Acknowledgment}

This research is supported by
the National Natural Science Foundation of China (Grant Nos. 61872039 and 61872167),
the Beijing Natural Science Foundation (Grant No. 4162040),
the Aeronautical Science Foundation of China (Grant No. 2016ZD74004), and
the Fundamental Research Funds for the Central Universities (Grant No. FRF-GF-17-B29).
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi
\bibliographystyle{IEEEtran}
\bibliography{DRT4WS}
\vspace{-6.7ex}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/authors/CASun.pdf}}]{Chang-ai Sun} is a Professor in the School of Computer and Communication Engineering, University of Science and Technology Beijing.
Before that, he was an Assistant Professor at Beijing Jiaotong University, China, a postdoctoral fellow at the Swinburne University of Technology, Australia, and a postdoctoral fellow at the University of Groningen, The Netherlands. He received the bachelor degree in Computer Science from the University of Science and Technology Beijing, China, and the PhD degree in Computer Science from Beihang University, China.
His research interests include software testing, program analysis, and Service-Oriented Computing.
\end{IEEEbiography}
\vspace{-6.7ex}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/authors/HPDai.jpg}}]{Hepeng Dai} is a PhD student in the School of Computer and Communication Engineering, University of Science and Technology Beijing, China. He received the master degree in Software Engineering from University of Science and Technology Beijing, China and the bachelor degree in Information and Computing Sciences from China University of Mining and Technology, Beijing. His current research interests include software testing and debugging.
\end{IEEEbiography}
\vspace{-6.7ex}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/authors/GWang.jpg}}]{Guan Wang} is a master student at the School of Computer and Communication Engineering, University of Science and Technology Beijing. He received a bachelor degree in Computer Science from University of Science and Technology Beijing. His current research interests include software testing and Service-Oriented Computing.
\end{IEEEbiography}
\vspace{-6.7ex}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/authors/DaveTowey.png}}]{Dave Towey} is an associate professor in the School of Computer Science, University of Nottingham Ningbo China. He received his BA and MA degrees from The University of Dublin, Trinity College, PgCertTESOL from The Open University of Hong Kong, MEd from The University of Bristol, and PhD from The University of Hong Kong. His current research interests include technology-enhanced teaching and learning, and software testing, especially metamorphic testing and adaptive random testing. He is a  member of both the IEEE and the ACM.
\end{IEEEbiography}
\vspace{-6.7ex}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/authors/TYChen.pdf}}]{Tsong Yueh Chen} is a Professor of Software Engineering at the Department of Computer Science and Software Engineering in Swinburne University of Technology. He received his PhD in Computer Science from The University of Melbourne, the MSc and DIC from Imperial College of Science and Technology, and BSc and MPhil from The University of Hong Kong. He is the inventor of metamorphic testing and adaptive random testing.
\end{IEEEbiography}
\vspace{-6.7ex}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/authors/KYCai.pdf}}]{Kai-Yuan Cai} received the BS, MS, and PhD degrees from Beihang university, Beijing, China, in 1984, 1987, and 1991, respectively. He has been a full professor at Beihang University since 1995. He is a Cheung Kong Scholar (chair professor) appointed by the Ministry of Education of China in 1999. His main research interests include software testing, software reliability, reliable flight control, and software cybernetics.
\end{IEEEbiography}

\end{document}
